# ğŸ¤– Transformer Modelleri GÃ¶revleri NasÄ±l Ã‡Ã¶zer?

## Genel BakÄ±ÅŸ

Transformer modelleri; metin (NLP), konuÅŸma/ses, gÃ¶rÃ¼ntÃ¼ ve diÄŸer modalitelerde Ã§eÅŸitli gÃ¶revleri Ã§Ã¶zmek iÃ§in kullanÄ±lÄ±r.  
Her gÃ¶revdeki farklÄ±lÄ±klar:
- Verinin nasÄ±l hazÄ±rlandÄ±ÄŸÄ±
- Hangi model mimarisinin kullanÄ±ldÄ±ÄŸÄ± (encoder, decoder, encoder-decoder)
- Ã‡Ä±ktÄ±nÄ±n nasÄ±l iÅŸlendiÄŸi



## ğŸ§  Transformer Mimarileri

### ğŸ”¹ Encoder  
Metni veya veriyi anlamaya odaklanÄ±r.  
**Ã–rnek:** BERT â€” metin sÄ±nÄ±flandÄ±rma, NER, soru-cevap

### ğŸ”¹ Decoder  
Yeni metin Ã¼retir.  
**Ã–rnek:** GPT â€” metin tamamlama, yazÄ± Ã¼retimi

### ğŸ”¹ Encoder-Decoder  
Bir veri tÃ¼rÃ¼nÃ¼ diÄŸerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (Ã§eviri, Ã¶zetleme).  
**Ã–rnek:** T5, BART â€” Ã§eviri, Ã¶zetleme, soru-cevap

![GPT-2 Mimarisi](./images/gpt2_architecture.png)



## ğŸ—£ï¸ Transformer TabanlÄ± Dil Modelleri

Dil modelleri, kelimeler arasÄ±ndaki iliÅŸkileri istatistiksel olarak Ã¶ÄŸrenerek insan dilini anlar ve Ã¼retir.

### Masked Language Modeling (MLM) â€“ Encoder tabanlÄ±  
BERT gibi modeller kullanÄ±r. MaskelenmiÅŸ kelimeleri baÄŸlamdan tahmin eder.

### Causal Language Modeling (CLM) â€“ Decoder tabanlÄ±  
GPT gibi modeller kullanÄ±r. Sadece Ã¶nceki kelimelere bakarak bir sonraki kelimeyi tahmin eder.



## ğŸ§© Model TÃ¼rleri ve GÃ¶revleri


**1. Encoder Modeller (BERT)**  
â¡ï¸ Metni analiz eder  
âš™ï¸KullanÄ±m alanlarÄ±:  
- SÄ±nÄ±flandÄ±rma
- Named Entity Recognition (NER)
- Soru-cevap

**2. Decoder Modeller (GPT, LLaMA)**  
â¡ï¸ Yeni metin Ã¼retir  
âš™ï¸KullanÄ±m alanlarÄ±:  
- Metin tamamlama
- Kod veya hikÃ¢ye Ã¼retimi

**3. Encoder-Decoder Modeller (T5, BART)**  
â¡ï¸ Girdi â†’ Ã§Ä±ktÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r  
âš™ï¸KullanÄ±m alanlarÄ±:  
- Ã–zetleme
- Ã‡eviri
- Soru-cevap


## âœï¸ Metin Ãœretimi (GPT-2)

- GPT-2 causal attention kullanÄ±r.
- Pozisyon bilgisiyle birlikte embedding uygulanÄ±r.
- Maskeli attention geleceÄŸe bakmayÄ± engeller.
- Ã‡Ä±ktÄ± dil modeli baÅŸlÄ±ÄŸÄ±na verilir â†’ sÄ±radaki kelime tahmin edilir.



## ğŸ—‚ï¸ Metin SÄ±nÄ±flandÄ±rma (BERT)

- WordPiece tokenizasyon
- [CLS] tokenâ€™Ä± sÄ±nÄ±flandÄ±rma iÃ§in
- Segment embedding ile Ã§ift cÃ¼mle ayrÄ±mÄ±
- MLM + NSP ile pretraining yapÄ±lÄ±r
- Lineer katman ile sÄ±nÄ±f tahmini yapÄ±lÄ±r



## ğŸ”¤ Token SÄ±nÄ±flandÄ±rma (NER / POS)

- Her tokenâ€™a ayrÄ± etiket verilir
- Final hidden state â†’ lineer katman â†’ token etiketi
- NER, POS tagging gibi gÃ¶revlerde kullanÄ±lÄ±r



## â“ Soru-Cevaplama

- CevabÄ±n baÅŸlangÄ±Ã§ ve bitiÅŸ pozisyonlarÄ± tahmin edilir
- Span classification: start + end logits
- Cross-entropy loss ile eÄŸitilir



## âœ‚ï¸ Ã–zetleme (BART)

- BART: encoder-decoder mimarisi
- Girdi bozulur â†’ decoder dÃ¼zeltir
- Text infilling: metin boÅŸluklarÄ± [mask] ile doldurulur
- Decoder orijinali tahmin eder



## ğŸŒ Ã‡eviri (BART / T5)

- Encoder, kaynak dili iÅŸler
- Decoder, hedef dili Ã¼retir
- mBART Ã§ok dilli destek sunar
- Cross-entropy loss ile eÄŸitilir


## ğŸ”Š Ses ve KonuÅŸma: Whisper

- 680.000 saat ses verisiyle eÄŸitildi
- Encoder: log-Mel spektrogram Ã¼retir â†’ Transformer encoder
- Decoder: metin Ã¼retir
- SÄ±fÄ±r-shot tanÄ±ma ve Ã§eviri yapabilir

```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition",
    model="openai/whisper-base.en"
)
transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

## ğŸ–¼ï¸ GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme: Vision Transformer (ViT)
- GÃ¶rsel, kÃ¼Ã§Ã¼k yamalara (patch) bÃ¶lÃ¼nÃ¼r
- Her patch â†’ vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
- [CLS] token eklenir
- Pozisyon bilgisi ile birlikte Transformer encoderâ€™a verilir
- Sadece [CLS] token Ã§Ä±ktÄ±sÄ± â†’ sÄ±nÄ±flandÄ±rma katmanÄ±na verilir
ViT â‰ˆ BERT
- [CLS] token
- Pozisyon kodlama
- Encoder bloklarÄ±

## ğŸ”š SonuÃ§
- NLP, gÃ¶rÃ¼ntÃ¼, ses gibi farklÄ± alanlarda Transformer mimarileri kullanÄ±lÄ±r
- Encoder â†’ anlamaya yÃ¶nelik gÃ¶revler
- Decoder â†’ Ã¼retim gÃ¶revleri
- Encoder-decoder â†’ dÃ¶nÃ¼ÅŸÃ¼m gÃ¶revleri (Ã§eviri, Ã¶zet)
- Transfer Ã¶ÄŸrenme sayesinde bu modeller pek Ã§ok gÃ¶reve kolayca adapte olabilir.
