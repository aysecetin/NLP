# ğŸ§  LLMâ€™lerde Metin Ãœretimi (Inference) Derinlemesine Ä°nceleme

## ğŸ” Inference Nedir?
LLMâ€™ler eÄŸitildikten sonra, onlardan Ã§Ä±ktÄ± almak iÃ§in kullanÄ±lan sÃ¼rece **inference (Ã§Ä±karÄ±m)** denir. Model, verilen bir input promptâ€™a gÃ¶re **birer birer token Ã¼reterek** yanÄ±t oluÅŸturur.

---

## ğŸ§² Dikkat MekanizmasÄ± (Attention)
LLMâ€™lerin baÄŸlamÄ± anlamasÄ±nÄ± ve tutarlÄ± metin Ã¼retmesini saÄŸlayan temel yapÄ± taÅŸÄ±dÄ±r. Ã–nemli kelimelere odaklanarak doÄŸru tahmin yapÄ±lmasÄ±nÄ± saÄŸlar.

> Ã–rnek: â€œFransaâ€™nÄ±n baÅŸkenti â€¦â€ â†’ â€œParisâ€  
Burada â€œFransaâ€ ve â€œbaÅŸkentiâ€ kelimeleri daha Ã¶nemlidir.

---

## ğŸ§® BaÄŸlam UzunluÄŸu (Context Length)
Modelin aynÄ± anda iÅŸleyebildiÄŸi maksimum token sayÄ±sÄ±dÄ±r. Uzun baÄŸlam daha anlamlÄ± metinler Ã¼retir, ancak:

- **Bellek kullanÄ±mÄ±**: baÄŸlam uzunluÄŸu arttÄ±kÃ§a **kareli (quadratic)** artar  
- **Ä°ÅŸlem sÃ¼resi**: doÄŸrusal (linear) artar

---

## âœï¸ Etkili Girdi HazÄ±rlama (Prompting)
Model sadece bir sonraki tokenâ€™Ä± tahmin eder. Bu yÃ¼zden **iyi yapÄ±landÄ±rÄ±lmÄ±ÅŸ promptlar**, Ã§Ä±ktÄ±nÄ±n kalitesini doÄŸrudan etkiler.

---

## ğŸ§± Inference SÃ¼reci: Ä°ki AÅŸama

### 1ï¸âƒ£ Prefill AÅŸamasÄ±
- **Tokenization**: Metin â†’ Tokenlar  
- **Embedding**: Tokenlar â†’ VektÃ¶rler  
- **Ä°lk Ä°ÅŸlem**: VektÃ¶rler â†’ Modelden geÃ§irilerek baÄŸlamsal temsil

> â›½ Bu aÅŸama Ã§ok yoÄŸundur, Ã§Ã¼nkÃ¼ tÃ¼m girdi bir defada iÅŸlenir.

### 2ï¸âƒ£ Decode AÅŸamasÄ±
Model birer birer token Ã¼retir. Her adÄ±mda:
- Ã–nceki tokenlara **dikkat** uygulanÄ±r  
- OlasÄ±lÄ±klar hesaplanÄ±r  
- Bir token seÃ§ilir  
- Devam edip etmeyeceÄŸi kontrol edilir

---

## ğŸ›ï¸ Token SeÃ§im Stratejileri

### ğŸ“‰ SÄ±caklÄ±k (Temperature)
- YÃ¼ksek deÄŸer (Ã¶r. 1.2): Daha rastgele ve yaratÄ±cÄ±
- DÃ¼ÅŸÃ¼k deÄŸer (Ã¶r. 0.7): Daha tutarlÄ± ve deterministik

### âœ… Top-k / Top-p Sampling
- **Top-k**: En olasÄ± k seÃ§enek arasÄ±ndan seÃ§im  
- **Top-p**: OlasÄ±lÄ±k toplamÄ± belirli bir eÅŸik (Ã¶r. %90) geÃ§ene kadar olanlar arasÄ±ndan seÃ§im

### ğŸ” Tekrardan KaÃ§Ä±nma
- **Presence penalty**: Daha Ã¶nce geÃ§en kelimelere ceza  
- **Frequency penalty**: SÄ±k kullanÄ±lanlara artan ceza

---

## ğŸ“ Metin UzunluÄŸu KontrolÃ¼
- **Token sÄ±nÄ±rlarÄ±**  
- **Durdurma dizileri (stop sequences)**  
- **EOS tokenÄ±** (Ã¶r. `<|im_end|>`)

---

## â™Ÿï¸ Beam Search
Modelin her adÄ±mda birden fazla olasÄ±lÄ±ÄŸÄ± paralel olarak deÄŸerlendirdiÄŸi tekniktir.

- 5â€“10 farklÄ± yol takip edilir  
- Her yol iÃ§in yeni tokenlar denenir  
- En yÃ¼ksek toplam olasÄ±lÄ±ÄŸa sahip zincir seÃ§ilir

> Daha iyi tutarlÄ±lÄ±k saÄŸlar, ancak hesaplama maliyeti yÃ¼ksektir.

---

## âš™ï¸ Performans ve Optimizasyon

### ğŸ”¢ Temel Metrikler
- **TTFT**: Ä°lk tokenâ€™Ä±n sÃ¼resi  
- **TPOT**: Her token Ã¼retim sÃ¼resi  
- **Throughput**: AynÄ± anda kaÃ§ istek yanÄ±tlanabilir  
- **VRAM kullanÄ±mÄ±**: Bellek gereksinimi

---

## ğŸ§  KV Cache ile HÄ±zlandÄ±rma
**KV caching**, Ã¶nceki adÄ±mlarda hesaplanan **Key-Value** deÄŸerlerini saklayarak:

- Tekrar hesaplamayÄ± engeller  
- Decode sÃ¼recini hÄ±zlandÄ±rÄ±r  
- Uzun baÄŸlamlarla Ã§alÄ±ÅŸmayÄ± mÃ¼mkÃ¼n kÄ±lar

> DezavantajÄ±: Bellek tÃ¼ketimi artar.

---

## âœ… SonuÃ§
LLM inference sÃ¼recini etkili kullanmak iÃ§in ÅŸu baÅŸlÄ±klarÄ± iyi kavramak gerekir:

- Attention ve baÄŸlamÄ±n rolÃ¼  
- Prefill ve Decode aÅŸamalarÄ±  
- Sampling stratejileri ve beam search  
- Performans darboÄŸazlarÄ± ve optimizasyonlar (KV Cache)

> ğŸ“Œ LLM dÃ¼nyasÄ± hÄ±zla geliÅŸiyor. Yeni teknikleri takip etmek ve farklÄ± senaryolarda denemeler yapmak kritik Ã¶nem taÅŸÄ±r.
