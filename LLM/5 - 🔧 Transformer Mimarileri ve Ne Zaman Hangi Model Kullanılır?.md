# ğŸ”§ Transformer Mimarileri ve Ne Zaman Hangi Model KullanÄ±lÄ±r?

Bu bÃ¶lÃ¼mde, Ã¼Ã§ temel Transformer mimarisini ve bunlarÄ±n hangi gÃ¶revler iÃ§in uygun olduÄŸunu inceleyeceÄŸiz.

---

## ğŸ§± ÃœÃ§ Temel Mimari

### 1. Encoder-Only Modeller
- Sadece encoder kÄ±smÄ± kullanÄ±lÄ±r.
- TÃ¼m cÃ¼mleyi aynÄ± anda gÃ¶rebilir (bidirectional attention).
- **KullanÄ±m AlanlarÄ±:**
  - Metin sÄ±nÄ±flandÄ±rma
  - Named Entity Recognition (NER)
  - Extractive soru-cevap
- **Ã–rnekler:** BERT, DistilBERT

---

### 2. Decoder-Only Modeller
- Sadece decoder kÄ±smÄ± kullanÄ±lÄ±r.
- Sadece Ã¶nceki kelimelere bakar (autoregressive).
- **KullanÄ±m AlanlarÄ±:**
  - Metin Ã¼retimi
  - Kod yazma
  - Chatbot uygulamalarÄ±
- **Ã–rnekler:** GPT, LLaMA, SmolLM, Gemma

---

### 3. Encoder-Decoder (Seq2Seq) Modeller
- Hem encoder hem decoder birlikte Ã§alÄ±ÅŸÄ±r.
- Encoder anlam Ã§Ä±karÄ±r, decoder Ã§Ä±ktÄ± Ã¼retir.
- **KullanÄ±m AlanlarÄ±:**
  - Ã‡eviri
  - Ã–zetleme
  - Generative soru-cevap
- **Ã–rnekler:** T5, BART, Marian, mBART

---

## ğŸ“Š GÃ¶reve GÃ¶re Mimari SeÃ§imi

| GÃ¶rev                         | Mimari            | Ã–rnek Modeller     |
|------------------------------|-------------------|--------------------|
| Metin sÄ±nÄ±flandÄ±rma          | Encoder           | BERT, RoBERTa      |
| Metin Ã¼retimi                | Decoder           | GPT, LLaMA         |
| Ã‡eviri                       | Encoder-Decoder   | T5, BART           |
| Ã–zetleme                     | Encoder-Decoder   | T5, BART           |
| Named Entity Recognition     | Encoder           | BERT, DistilBERT   |
| Soru-cevap (extractive)      | Encoder           | BERT               |
| Soru-cevap (generative)      | Encoder-Decoder   | T5, BART           |
| Sohbet (chatbot)             | Decoder           | GPT, LLaMA         |

---

## ğŸŒŸ Modern LLMâ€™ler: Ã–zellikler ve AÅŸamalar

- **Pretraining:** BÃ¼yÃ¼k metinlerle sonraki token'Ä± tahmin eder.
- **Instruction Tuning:** Ä°nsan gibi yanÄ±t verecek ÅŸekilde ince ayar yapÄ±lÄ±r.

**Yetenekleri:**
- Metin Ã¼retimi
- Ã–zetleme
- Ã‡eviri
- Kod Ã¼retimi
- Soru yanÄ±tlama
- AkÄ±l yÃ¼rÃ¼tme
- Few-shot Ã¶ÄŸrenme

---

## âš™ï¸ Verimli Attention YÃ¶ntemleri

### ğŸ”¹ LSH Attention (Reformer)
- Sadece ilgili kelimelere dikkat eder.
- Hashing ile benzer kelimeler seÃ§ilir.

### ğŸ”¹ Local Attention (Longformer)
- Sadece komÅŸu kelimeler dikkate alÄ±nÄ±r.
- BazÄ± Ã¶zel token'lara global dikkat verilir.

### ğŸ”¹ Axial Positional Encoding
- Pozisyon bilgisi bÃ¼yÃ¼k matris yerine iki kÃ¼Ã§Ã¼k matrisle hesaplanÄ±r.
- Bellek kullanÄ±mÄ±nÄ± azaltÄ±r.

---

## âœ… Ã–zet

- GÃ¶rev tÃ¼rÃ¼ne gÃ¶re doÄŸru mimari seÃ§imi baÅŸarÄ± iÃ§in kritik:
  - Anlama gÃ¶revleri â†’ **Encoder**
  - Ãœretim gÃ¶revleri â†’ **Decoder**
  - Girdi-Ã§Ä±ktÄ± dÃ¶nÃ¼ÅŸÃ¼mleri â†’ **Encoder-Decoder**
- Modern LLMâ€™ler Ã§oÄŸunlukla decoder mimarisiyle Ã§alÄ±ÅŸÄ±r.
- Uzun metinlerde Ã¶zel attention stratejileri performansÄ± artÄ±rÄ±r.

