
# ğŸ“š Birden Fazla Dizi (Sequence) ile Ã‡alÄ±ÅŸmak

Ã–nceki bÃ¶lÃ¼mde yalnÄ±zca **tek bir kÄ±sa cÃ¼mle** ile nasÄ±l Ã§Ä±karÄ±m (inference) yapÄ±lacaÄŸÄ±nÄ± gÃ¶rdÃ¼k. Åimdi ÅŸu sorular ortaya Ã§Ä±kÄ±yor:

- Birden fazla cÃ¼mleyi nasÄ±l iÅŸleriz?
- UzunluklarÄ± farklÄ± olan cÃ¼mlelerle ne yaparÄ±z?
- Sadece `input_ids` yeterli mi?
- Uzunluk Ã§ok fazla olursa ne olur?



### ğŸ§¾ Modeller Toplu (Batch) GiriÅŸ Bekler

AÅŸaÄŸÄ±daki gibi bir cÃ¼mleyi doÄŸrudan modele verirsek hata alÄ±rÄ±z:

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)

# HATA: Tensor boyutu eksik
model(input_ids)
```

### âœ… DoÄŸru kullanÄ±m:

```python
input_ids = torch.tensor([ids])  # Yeni bir boyut ekleniyor
output = model(input_ids)
print(output.logits)
```

Bu Ã§Ä±ktÄ±lar "logits"tir â€” modelin tahmin skorlarÄ±dÄ±r.



### ğŸ“¦ Toplu Ä°ÅŸlem (Batching)

AynÄ± cÃ¼mleyi iki kez gÃ¶nderelim:
```python
batched_ids = [ids, ids]
```

Bu artÄ±k 2 cÃ¼mlelik bir batch!

> âœï¸ Uygulama: Bu listeyi tensÃ¶re Ã§evirip modelden Ã§Ä±ktÄ±yÄ± al. Logitâ€™lerin aynÄ± olduÄŸunu gÃ¶receksin.



### ğŸ”² Pad Etme (Padding)

FarklÄ± uzunluktaki diziler doÄŸrudan tensÃ¶re Ã§evrilemez. Ã–rneÄŸin:

```python
batched_ids = [
    [200, 200, 200],
    [200, 200]
]
```

Bunu dÃ¼zeltmek iÃ§in `padding` yapÄ±lÄ±r:

```python
padding_id = tokenizer.pad_token_id

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id]
]
```

Ama bu haliyle model Ã§Ä±ktÄ±larÄ± **bozulur**, Ã§Ã¼nkÃ¼ Transformerâ€™lar tÃ¼m tokenâ€™lara dikkat eder (padâ€™ler dahil!).



### ğŸ§  Dikkat Maskesi (Attention Mask)

Pad edilen tokenâ€™larÄ± **yok saymak** iÃ§in attention maskesi kullanÄ±lÄ±r:

```python
attention_mask = [
    [1, 1, 1],
    [1, 1, 0]
]

outputs = model(
    torch.tensor(batched_ids),
    attention_mask=torch.tensor(attention_mask)
)
print(outputs.logits)
```

Bu sayede padâ€™li sonuÃ§ ile tekli sonuÃ§ **aynÄ± Ã§Ä±kar**.

### âœï¸ Uygulama Ã–nerisi

Åu iki cÃ¼mleyi elle tokenize edip modele ver:

- "I've been waiting for a HuggingFace course my whole life."
- "I hate this so much!"

Ã–nce tek tek ver, sonra padâ€™leyip batch halinde ver. AynÄ± sonuÃ§larÄ± aldÄ±ÄŸÄ±nÄ± gÃ¶r.



### ğŸ“ Uzun Sekanslar Ne Olur?

Ã‡oÄŸu Transformer modeli **maksimum 512 veya 1024 token**'Ä± destekler. Daha uzun diziler hata verir.

**2 Ã§Ã¶zÃ¼m:**

1. **Uzun dizilerle baÅŸa Ã§Ä±kan modeller** kullan (Ã¶rneÄŸin: Longformer, LED)
2. **KÄ±saltma (Truncation)** uygula:

```python
sequence = sequence[:max_sequence_length]
```



Transformer modellerinde batching, padding, ve attention maskeleri doÄŸru kullanmak performans ve doÄŸruluk iÃ§in kritik Ã¶neme sahiptir.
