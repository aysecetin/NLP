# ğŸ¤— Transformers ile Duygu Analizi â€“ AdÄ±m AdÄ±m AÃ§Ä±klama

### ğŸ“Œ BaÅŸlangÄ±Ã§

Ä°lk bÃ¶lÃ¼mde kullandÄ±ÄŸÄ±mÄ±z kod ÅŸu ÅŸekildeydi:

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier([
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
])
```

Bu kodun Ã§Ä±ktÄ±sÄ±:

```python

[{'label': 'POSITIVE', 'score': 0.9598},
 {'label': 'NEGATIVE', 'score': 0.9994}]
```

Bu basit pipelineaslÄ±nda Ã¼Ã§ aÅŸamayÄ± kapsar:

- Preprocessing (Ã–n iÅŸleme)

- Model Inference (Modelden geÃ§irme)

- Postprocessing (Son iÅŸleme)

### âœ‚ï¸ 1. Ã–n Ä°ÅŸleme (Tokenizer ile)
- Transformer modelleri ham metni anlayamaz. Ã–nce bu metinleri sayÄ±lara (tokenâ€™lara) dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir.

**Tokenizer Ne Yapar?**
- Metni kelime, alt-kelime veya sembollere bÃ¶ler

- Her tokenâ€™a bir sayÄ± atar (input_ids)

- Ek bilgiler (Ã¶rneÄŸin attention_mask) ekler

```python

from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]


inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
```
DÃ¶nen sonuÃ§:

```python
{
  'input_ids': tensor([[...], [...]]),
  'attention_mask': tensor([[1, 1, ..., 1], [1, 1, ..., 0]])
}
```
- input_ids: CÃ¼mledeki tokenâ€™larÄ±n sayÄ±sal temsili

- attention_mask: Modelin hangi tokenlara dikkat etmesi gerektiÄŸini belirtir

### ğŸ§  2. Modelden GeÃ§irme (Inference)
Modeli yÃ¼klÃ¼yoruz:

```python

from transformers import AutoModel

model = AutoModel.from_pretrained(checkpoint)
```
Modelin Ã§Ä±ktÄ±sÄ± hidden states adÄ± verilen gizli temsillerdir:

```python

outputs = model(**inputs)
print(outputs.last_hidden_state.shape)  # torch.Size([2, 16, 768])
```
- 2 cÃ¼mle â†’ 2 Ã¶rnek (batch size)

- 16 token â†’ sequence length

- 768 â†’ her token iÃ§in vektÃ¶r boyutu (hidden size)

### ğŸ¯ 3. Model Head: SÄ±nÄ±flandÄ±rma
Duygu analizi iÃ§in sÄ±nÄ±flandÄ±rma baÅŸlÄ±ÄŸÄ± olan bir model gereklidir:

```python

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
print(outputs.logits.shape)  # torch.Size([2, 2])
```
Her cÃ¼mle iÃ§in 2 skor (logit) dÃ¶ner: NEGATIVE ve POSITIVE.

### ğŸ”„ 4. Son Ä°ÅŸleme (Postprocessing)
Modelin Ã§Ä±ktÄ±sÄ± logit yani normalize edilmemiÅŸ skorlardÄ±r:

```python

print(outputs.logits)
# tensor([[-1.5607,  1.6123],
#         [ 4.1692, -3.3464]])
```
Bu skorlarÄ± Softmax fonksiyonu ile olasÄ±lÄ±ÄŸa Ã§eviriyoruz:

```python

import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
# tensor([[0.0402, 0.9598],
#         [0.9995, 0.0005]])
```
Etiketleri almak iÃ§in:

```python

print(model.config.id2label)
# {0: 'NEGATIVE', 1: 'POSITIVE'}
```
SonuÃ§:

- Ä°lk cÃ¼mle â†’ %95.98 olasÄ±lÄ±kla POZÄ°TÄ°F

- Ä°kinci cÃ¼mle â†’ %99.95 olasÄ±lÄ±kla NEGATÄ°F

### âœ… Ã–zet: NLP Pipelineâ€™Ä±n 3 Temel AÅŸamasÄ±
- Preprocessing â†’ Tokenizer ile metni sayÄ±lara Ã§evir

- Model Inference â†’ Model Ã¼zerinden tahmin yap

- Postprocessing â†’ SonuÃ§larÄ± olasÄ±lÄ±ÄŸa Ã§evir ve etiketle eÅŸleÅŸtir

