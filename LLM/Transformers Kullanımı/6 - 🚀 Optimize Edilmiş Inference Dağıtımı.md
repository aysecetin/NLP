
# ğŸš€ Optimize EdilmiÅŸ Inference DaÄŸÄ±tÄ±mÄ±

Bu bÃ¶lÃ¼mde bÃ¼yÃ¼k dil modellerini (LLM) **Ã¼retim ortamÄ±nda** verimli bir ÅŸekilde sunmak iÃ§in kullanÄ±lan Ã¼Ã§ gÃ¼Ã§lÃ¼ frameworkâ€™Ã¼ inceleyeceÄŸiz:

- **TGI (Text Generation Inference)**
- **vLLM**
- **llama.cpp**

Bu sistemler; yÃ¼ksek verimlilik, hafÄ±za yÃ¶netimi, esneklik ve Ã¼retime kolay entegrasyon aÃ§Ä±sÄ±ndan farklÄ± avantajlar sunar.



### âš™ï¸ Framework KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Ã–zellik | TGI | vLLM | llama.cpp |
|--------|-----|------|-----------|
| ğŸš€ Performans | Flash Attention 2 + sÃ¼rekli batchleme | PagedAttention ile KV cache optimizasyonu | CPU iÃ§in optimizasyon + quantization |
| ğŸ§  Bellek KullanÄ±mÄ± | Sabit uzunluklu sekanslarla tahmin edilebilir | Sayfa tabanlÄ± bellek yÃ¶netimi | 2-bitâ€™e kadar quantization |
| ğŸ“¦ DaÄŸÄ±tÄ±m | Kubernetes desteÄŸi, Prometheus, Grafana | OpenAI API uyumu, Ray entegrasyonu | Minimal, C/C++ tabanlÄ±, portatif |
| ğŸ§± DonanÄ±m DesteÄŸi | GPU zorunlu | GPU | CPU aÄŸÄ±rlÄ±klÄ±, opsiyonel GPU |



### ğŸ’¡ TGI: Flash Attention 2

- **Flash Attention**, belleÄŸe veri transferini minimuma indirerek GPUâ€™yu daha verimli kullanÄ±r.
- Bellek taÅŸmalarÄ± Ã¶nlenir, VRAM verimli kullanÄ±lÄ±r.
- SÃ¼rekli batchleme ile GPU hep meÅŸgul tutulur.



### ğŸ” vLLM: PagedAttention

- **PagedAttention**, KV cache yÃ¶netiminde â€œsayfalamaâ€ yaparak bellek parÃ§alanmasÄ±nÄ± azaltÄ±r.
- Bellek paylaÅŸÄ±mÄ± sayesinde paralel Ã¶rnekler daha verimli iÅŸlenir.
- 24 kata kadar throughput artÄ±ÅŸÄ± saÄŸlanabilir.



### ğŸ§® llama.cpp: Quantization + CPU Optimizasyonu

- 32-bit FP yerine 8-bit, 4-bit, 2-bit gibi quantization ile daha az bellek kullanÄ±mÄ±
- GGUF formatÄ± ile CPU dostu inference
- AVX2, AVX-512 gibi CPU mimarileri iÃ§in optimize edilmiÅŸ



### ğŸ§ª Kurulum ve Sunucu BaÅŸlatma

### ğŸ”¹ TGI

```bash
docker run --gpus all -p 8080:80     -v ~/.cache/huggingface:/data     ghcr.io/huggingface/text-generation-inference:latest     --model-id HuggingFaceTB/SmolLM2-360M-Instruct
```

### ğŸ”¹ vLLM

```bash
python -m vllm.entrypoints.openai.api_server \
    --model HuggingFaceTB/SmolLM2-360M-Instruct \
    --host 0.0.0.0 \
    --port 8000
```

### ğŸ”¹ llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

./server -m model.gguf --port 8080
```



### ğŸ§µ Metin Ãœretimi (Text Generation)

TÃ¼m frameworkâ€™ler hem **chat** hem de **text generation** iÃ§in kullanÄ±lÄ±r:

**Ã–rnek (Hugging Face Client ile):**

```python
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://localhost:8080")

response = client.text_generation(
    "Tell me a story",
    max_new_tokens=100,
    temperature=0.8,
    top_p=0.95,
    repetition_penalty=1.1,
    details=True,
)
print(response.generated_text)
```

---

### ğŸ›ï¸ GeliÅŸmiÅŸ Kontroller

**Sampling AyarlarÄ±:**

- `temperature`: Rastgelelik (yÃ¼ksekse daha yaratÄ±cÄ±)
- `top_p`: Belirli olasÄ±lÄ±k eÅŸiÄŸine kadar olan tokenâ€™lar
- `top_k`: En olasÄ± k tokenâ€™dan seÃ§im

**Tekrardan KaÃ§Ä±nma:**

- `repetition_penalty`: AynÄ± token tekrarÄ±nÄ± engeller
- `frequency_penalty`: SÄ±k gelen tokenâ€™lara ceza
- `presence_penalty`: Ã–nceki tokenâ€™lara ceza

**Uzunluk & Durdurma:**

- `max_tokens`: Maksimum Ã¼retim uzunluÄŸu
- `stop_sequences`: Belirli diziler geldiÄŸinde durdur



### ğŸ’¾ Bellek YÃ¶netimi

### TGI

```bash
docker run --gpus all -p 8080:80 \
    --shm-size 1g \
    ghcr.io/huggingface/text-generation-inference:latest \
    --model-id ... \
    --max-batch-total-tokens 8192 \
    --max-input-length 4096
```

### llama.cpp

```bash
./server \
    -m model.gguf \
    --n-gpu-layers 20 \
    --mlock \
    --cont-batching
```

### vLLM

```python
from vllm.engine.arg_utils import AsyncEngineArgs

engine_args = AsyncEngineArgs(
    model="...",
    gpu_memory_utilization=0.85,
    max_num_batched_tokens=8192,
    block_size=16,
)

llm = LLM(engine_args=engine_args)
```

Bu Ã¼Ã§ sistem de farklÄ± senaryolar iÃ§in idealdir:

- **TGI**: Kurumsal, stabil, izlenebilirlik yÃ¼ksek daÄŸÄ±tÄ±mlar iÃ§in
- **vLLM**: Esnek, performans odaklÄ±, API uyumlu sistemler iÃ§in
- **llama.cpp**: DonanÄ±mÄ± kÄ±sÄ±tlÄ±, taÅŸÄ±nabilir yerel uygulamalar iÃ§in

