

# ğŸ“Œ Modeli Trainer API ile Ä°nce Ayar (Fine-Tuning)

ğŸ¤— Transformers, Ã¶nceden eÄŸitilmiÅŸ modelleri modern yÃ¶ntemlerle kolayca ince ayar yapabilmenizi saÄŸlayan `Trainer` adlÄ± bir sÄ±nÄ±f sunar.

## ğŸ”§ Gerekli HazÄ±rlÄ±klar

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

## âš™ï¸ EÄŸitim AyarlarÄ±

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

GPU kullanmÄ±yorsan eÄŸitim Ã§ok yavaÅŸ olabilir. Google Colab veya benzeri platformlarda Ã¼cretsiz GPU'larla Ã§alÄ±ÅŸabilirsin.

## ğŸ§  Model TanÄ±mÄ±

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

Modelin baÅŸ kÄ±smÄ± gÃ¶revimize uygun hale getirilir; bazÄ± aÄŸÄ±rlÄ±klar rastgele baÅŸlatÄ±lÄ±r.

## ğŸ‹ï¸ Trainer SÄ±nÄ±fÄ±nÄ± OluÅŸturma

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    processing_class=tokenizer,
)
```

`processing_class` ile hangi tokenizer'Ä±n kullanÄ±lacaÄŸÄ± belirtilir.

## ğŸš€ EÄŸitimi BaÅŸlatmak

```python
trainer.train()
```

EÄŸitim sÄ±rasÄ±nda sadece kayÄ±p (loss) deÄŸeri raporlanÄ±r. DoÄŸruluk (accuracy) gibi metrikler iÃ§in ayrÄ±ca `compute_metrics()` tanÄ±mlamak gerekir.

## ğŸ“Š DeÄŸerlendirme Metrikleri

```python
import evaluate
import numpy as np

metric = evaluate.load("glue", "mrpc")

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

Trainerâ€™a yeniden bu fonksiyonu tanÄ±t:

```python
training_args = TrainingArguments("test-trainer", eval_strategy="epoch")

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    processing_class=tokenizer,
    compute_metrics=compute_metrics,
)
trainer.train()
```

## âš¡ï¸ GeliÅŸmiÅŸ Ã–zellikler

- **Mixed Precision (fp16):** Bellek kullanÄ±mÄ±nÄ± azaltÄ±r, eÄŸitimi hÄ±zlandÄ±rÄ±r.
- **Gradient Accumulation:** KÃ¼Ã§Ã¼k batchâ€™lerle bÃ¼yÃ¼k batch etkisi saÄŸlar.
- **Learning Rate Scheduling:** Ã–ÄŸrenme oranÄ± zamanla azaltÄ±labilir (`lr_scheduler_type`).



**Soru-Cevap:**

1. `processing_class` ne iÅŸe yarar?  
   â†’ Tokenizerâ€™Ä± belirtmek iÃ§in kullanÄ±lÄ±r.

2. `eval_strategy` neyi kontrol eder?  
   â†’ DeÄŸerlendirmenin sÄ±klÄ±ÄŸÄ±nÄ± belirler (adÄ±m adÄ±m veya epoch sonunda).

3. `fp16=True` ne saÄŸlar?  
   â†’ Mixed precision ile daha az bellek, daha hÄ±zlÄ± eÄŸitim.

4. `compute_metrics()` fonksiyonu ne iÅŸe yarar?  
   â†’ Logit'leri tahmine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, doÄŸruluk ve F1 gibi metrikleri hesaplar.

5. `eval_dataset` verilmezse ne olur?  
   â†’ EÄŸitim yapÄ±lÄ±r ama deÄŸerlendirme metrikleri alÄ±nmaz.

6. Gradient accumulation nedir?  
   â†’ BirkaÃ§ batch'in gradyanlarÄ±nÄ± biriktirip sonra aÄŸÄ±rlÄ±klarÄ± gÃ¼nceller.


Trainer API, tÃ¼m bu sÃ¼reci yÃ¼ksek seviyede kontrol etmeni saÄŸlayarak hÄ±zlÄ± ve gÃ¼venilir model ince ayarÄ± saÄŸlar.

## ğŸ’¡ Ã–nemli Noktalar:

- API Trainer, Ã§oÄŸu eÄŸitim karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± ele alan Ã¼st dÃ¼zey bir arayÃ¼z saÄŸlar
- processing_classUygun veri iÅŸleme iÃ§in belirteÃ§leyicinizi belirtmek iÃ§in kullanÄ±n
- TrainingArgumentseÄŸitimin tÃ¼m yÃ¶nlerini kontrol eder: Ã¶ÄŸrenme hÄ±zÄ±, parti bÃ¼yÃ¼klÃ¼ÄŸÃ¼, deÄŸerlendirme stratejisi ve optimizasyonlar
-  compute_metricsyalnÄ±zca eÄŸitim kaybÄ±nÄ±n Ã¶tesinde Ã¶zel deÄŸerlendirme Ã¶lÃ§Ã¼mlerini mÃ¼mkÃ¼n kÄ±lar
- Karma hassasiyet ( ) ve gradyan birikimi gibi modern Ã¶zellikler, fp16=TrueeÄŸitim verimliliÄŸini Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rabilir
