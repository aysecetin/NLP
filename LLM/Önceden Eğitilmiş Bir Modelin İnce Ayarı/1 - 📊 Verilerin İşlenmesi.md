# ğŸ“Š Verilerin Ä°ÅŸlenmesi

## ğŸ”¢ Tokenizer ile Veri HazÄ±rlÄ±ÄŸÄ±

Bir tokenizer, cÃ¼mleleri modele uygun sayÄ±sal tensÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lÄ±r. Ã–rnek olarak, `"bert-base-uncased"` modeli ile cÃ¼mle Ã§iftleri token'lanÄ±r. Bu iÅŸlem:
- Padding (doldurma),
- Truncation (kÄ±saltma),
- Attention mask ve token type ID Ã¼retimini kapsar.

## ğŸ“š MRPC Dataset Ã–rneÄŸi

GLUE benchmark'Ä±ndaki MRPC veri kÃ¼mesi 5801 cÃ¼mle Ã§ifti iÃ§erir. CÃ¼mlelerin eÅŸdeÄŸer (paraphrase) olup olmadÄ±ÄŸÄ±nÄ± gÃ¶steren etiketler iÃ§erir. `load_dataset("glue", "mrpc")` ile kolayca yÃ¼klenebilir.

Dataset yÃ¼kledikten sonra `sentence1`, `sentence2`, `label` gibi sÃ¼tunlara eriÅŸebilirsiniz.

## âœ‚ï¸ Tokenization & Ã–n Ä°ÅŸleme

Model iki cÃ¼mleyi birlikte iÅŸleyecekse, tokenizer iki cÃ¼mleyi tek seferde alabilir:
```python
tokenizer("Bu ilk cÃ¼mledir.", "Bu ikinci cÃ¼mledir.")
```
Bu iÅŸlem `input_ids`, `attention_mask`, ve `token_type_ids` dÃ¶ndÃ¼rÃ¼r.  
- `token_type_ids` her tokenâ€™Ä±n hangi cÃ¼mleye ait olduÄŸunu belirtir.  
- BazÄ± modeller (Ã¶rneÄŸin DistilBERT) bunu desteklemez.

## ğŸ› ï¸ Dataset.map() ile TokenleÅŸtirme

Veri kÃ¼mesini dÃ¶nÃ¼ÅŸtÃ¼rmenin en verimli yolu `Dataset.map()` fonksiyonudur.  
```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```
Bu yÃ¶ntem, belleÄŸe tÃ¼m dataset'i yÃ¼klemeye gerek kalmadan Ã§alÄ±ÅŸÄ±r.

## ğŸ§© Dinamik Padding

Daha verimli eÄŸitim iÃ§in dinamik padding Ã¶nerilir. Bunun iÃ§in `DataCollatorWithPadding` kullanÄ±lÄ±r:
```python
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```
Bu yapÄ±, batch iÃ§erisindeki en uzun girdiye gÃ¶re padding yapar.

## ğŸš€ Ã–nemli Noktalar

- `batched=True` ile `Dataset.map()` iÅŸlemleri hÄ±zlandÄ±rÄ±lÄ±r.
- Dinamik padding, sabit paddingâ€™e gÃ¶re daha verimlidir.
- Veri iÅŸleme Ã§Ä±ktÄ±sÄ±, modelin beklediÄŸi formatta olmalÄ±dÄ±r.
