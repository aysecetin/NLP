# ğŸš€ Transformer Modelleri Nedir ve NasÄ±l Ã‡alÄ±ÅŸÄ±r?

## 1. Transformer Nedir?

Transformer, 2017 yÄ±lÄ±nda "Attention is All You Need" adlÄ± makaleyle tanÄ±tÄ±lan bir yapay zeka model mimarisidir. Ã–zellikle doÄŸal dil iÅŸleme (NLP) alanÄ±nda bÃ¼yÃ¼k bir etki yaratmÄ±ÅŸtÄ±r.

DiÄŸer modellerin aksine, Transformer'lar sÄ±ralÄ± veriyle Ã§alÄ±ÅŸÄ±rken verileri sÄ±rayla iÅŸlemez. Bunun yerine, tÃ¼m kelimelere aynÄ± anda bakar ve dikkat (attention) mekanizmasÄ±yla hangi kelimelere odaklanmasÄ± gerektiÄŸini belirler. Bu sayede uzun cÃ¼mlelerde bile baÄŸlamÄ± koruyarak daha baÅŸarÄ±lÄ± sonuÃ§lar Ã¼retir.



## 2. Ana YapÄ±sÄ± NasÄ±ldÄ±r?

Transformer iki temel bileÅŸenden oluÅŸur:

- **Encoder (KodlayÄ±cÄ±):** Girdi metnini iÅŸler ve anlamlÄ± temsiller (Ã¶zellikler) oluÅŸturur.
- **Decoder (Kod Ã‡Ã¶zÃ¼cÃ¼):** Encoder Ã§Ä±ktÄ±sÄ±nÄ± kullanarak hedef metni Ã¼retir (Ã¶rneÄŸin Ã§eviri yaparken).

Bu yapÄ± Ã¼Ã§ farklÄ± kullanÄ±m ÅŸeklinde karÅŸÄ±mÄ±za Ã§Ä±kar:

- **Encoder-Only:** GiriÅŸin analiz edilmesi gereken gÃ¶revlerde kullanÄ±lÄ±r (Ã¶rneÄŸin duygu analizi, metin sÄ±nÄ±flandÄ±rma).
- **Decoder-Only:** Metin Ã¼retimi gibi gÃ¶revlerde kullanÄ±lÄ±r (Ã¶rneÄŸin GPT modelleri).
- **Encoder-Decoder:** GiriÅŸten bir Ã§Ä±ktÄ± Ã¼retildiÄŸi durumlarda kullanÄ±lÄ±r (Ã¶rneÄŸin Ã§eviri, Ã¶zetleme).


## 3. Dikkat (Attention) MekanizmasÄ±

Attention, Transformerâ€™Ä±n temelidir. Modelin, cÃ¼mledeki hangi kelimenin diÄŸer kelimelerle olan iliÅŸkisine daha fazla "dikkat" etmesi gerektiÄŸini belirler.

Ã–rneÄŸin:  
"You like this course" cÃ¼mlesini FransÄ±zcaya Ã§evirirken, "like" fiilinin doÄŸru Ã§ekimi iÃ§in "you" Ã¶znesine dikkat edilmelidir. AynÄ± ÅŸekilde, "this" kelimesi Ã§evrilirken "course" kelimesinin cinsiyetine bakmak gerekebilir.

Bu sayede model, baÄŸlama duyarlÄ± Ã§eviriler ve analizler yapabilir. Bu mekanizma her kelime iÃ§in tÃ¼m diÄŸer kelimeleri deÄŸerlendirir ve baÄŸlamÄ± dikkate alarak karar verir.



## 4. Transformer Ailesi ve Ã–rnek Modeller

YÄ±llar iÃ§inde farklÄ± Ã¶zellikler taÅŸÄ±yan birÃ§ok Transformer tabanlÄ± model geliÅŸtirildi:

- **GPT (2018):** Metin Ã¼retiminde baÅŸarÄ±lÄ±. Tek yÃ¶nlÃ¼, sadece geÃ§miÅŸe bakar (causal).
- **BERT (2018):** CÃ¼mle anlamÄ± Ã¼zerine gÃ¼Ã§lÃ¼. Maskeli kelime tahmini yapar (bidirectional).
- **GPT-2 / GPT-3 / InstructGPT (2019-2022):** Daha bÃ¼yÃ¼k, daha gÃ¼Ã§lÃ¼ versiyonlar. Ã–zellikle GPT-3 sÄ±fÄ±r Ã¶rnekle (zero-shot) gÃ¶revleri gerÃ§ekleÅŸtirebilir.
- **T5 (2019):** TÃ¼m gÃ¶revleri bir Ã§eviri problemine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
- **LLaMA, Mistral, Gemma, SmolLM (2023â€“2024):** Daha verimli, hafif ve bazÄ±larÄ± Ã§ok dilli yeni nesil modellerdir.

Bu modellerin Ã§oÄŸu Ã¼Ã§ ana gruba ayrÄ±lÄ±r:
- **GPT-tÃ¼rÃ¼ (oto-regresif)**
- **BERT-tÃ¼rÃ¼ (oto-ÅŸifreleyici)**
- **T5-tÃ¼rÃ¼ (giriÅŸten Ã§Ä±kÄ±ÅŸa - sequence-to-sequence)**

---

## 5. Model EÄŸitimi: Pretraining ve Fine-Tuning

Transformer modelleri iki aÅŸamada eÄŸitilir:

### ğŸ”§ Pretraining (Ã–n EÄŸitim):
- BÃ¼yÃ¼k miktarda etiketsiz metinle model sÄ±fÄ±rdan eÄŸitilir.
- Model dilin istatistiksel yapÄ±sÄ±nÄ± Ã¶ÄŸrenir.
- Bu sÃ¼reÃ§ Ã§ok zaman ve kaynak gerektirir (Ã¶rneÄŸin haftalar sÃ¼rebilir).

### ğŸ”§ Fine-Tuning (Ä°nce Ayar):
- Ã–nceden eÄŸitilmiÅŸ model, daha kÃ¼Ã§Ã¼k ve etiketli bir veriyle Ã¶zel bir gÃ¶reve gÃ¶re ayarlanÄ±r.
- Daha az veri, daha az zaman ve daha az kaynakla iyi sonuÃ§lar elde edilir.
- Bu nedenle her zaman hazÄ±r bir modelle baÅŸlamak, sÄ±fÄ±rdan eÄŸitimden Ã§ok daha avantajlÄ±dÄ±r.

Ã–rneÄŸin: Ä°ngilizce eÄŸitilmiÅŸ bir modeli, bilimsel makaleleri anlamasÄ± iÃ§in arXiv veri kÃ¼mesiyle fine-tune edebiliriz.

---

## 6. Transformerâ€™lar BÃ¼yÃ¼k ve GÃ¼Ã§lÃ¼dÃ¼r

Transformer modellerinin baÅŸarÄ±sÄ±, genellikle boyutlarÄ±nÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼yle paraleldir.  
GPT-3 gibi modellerin yÃ¼z milyarlarca parametresi vardÄ±r. Bu da eÄŸitimlerini:

- **Zaman aÃ§Ä±sÄ±ndan pahalÄ±**,  
- **DonanÄ±m aÃ§Ä±sÄ±ndan zor**,  
- **Ã‡evresel olarak maliyetli** hÃ¢le getirir.

Bu yÃ¼zden modellerin aÄŸÄ±rlÄ±klarÄ±nÄ±n paylaÅŸÄ±lmasÄ± Ã§ok Ã¶nemlidir. BÃ¶ylece bir modeli sÄ±fÄ±rdan eÄŸitmek yerine, hazÄ±r bir modelin Ã¼zerine inÅŸa etmek hem Ã§evre dostu hem de verimlidir.

AyrÄ±ca araÃ§lar sayesinde eÄŸitimin karbon ayak izi hesaplanabilir:  
- [`ML CO2 Impact`](https://mlco2.github.io/impact)  
- [`CodeCarbon`](https://codecarbon.io) (ğŸ¤— Transformers ile entegre Ã§alÄ±ÅŸÄ±r)

---

## 7. Terimler

BazÄ± temel terimler:

- **Architecture (Mimari):** Modelin iskeleti, yani katmanlar ve yapÄ± (Ã¶rneÄŸin: BERT).
- **Checkpoint:** O mimari Ã¼zerinde eÄŸitilmiÅŸ Ã¶zel aÄŸÄ±rlÄ±klar (Ã¶rneÄŸin: `bert-base-cased`).
- **Model:** Genel bir terim; bazen mimariyi, bazen checkpointâ€™i veya ikisini birden ifade eder.

Ã–rneÄŸin:
- "BERT" â†’ bir mimaridir.  
- "bert-base-cased" â†’ Googleâ€™Ä±n bu mimariyle eÄŸittiÄŸi aÄŸÄ±rlÄ±klardÄ±r.  
- "BERT modeli" â†’ ikisini de kast edebilir.

---

ğŸ“Œ Bu temel bilgiler Ä±ÅŸÄ±ÄŸÄ±nda Transformer mimarilerinin detaylarÄ±na daha sonra daha derinlemesine dalacaÄŸÄ±z. Åimdilik genel yapÄ±yÄ± anlamanÄ±z yeterli.

